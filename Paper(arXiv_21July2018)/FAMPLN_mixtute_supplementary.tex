\documentclass[12pt]{article}      
\usepackage{epsfig,multirow,setspace}

\AtEndOfClass{\RequirePackage{times}}
\textwidth=165mm \headheight=0mm \headsep=10mm \topmargin=0mm
\textheight=210mm %\footskip=1.5cm
\oddsidemargin=0mm
% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

%\usepackage{jmlr2e}
\usepackage{mathtools}
\usepackage{amsthm,amsmath,amssymb,colonequals,multirow,graphicx,epstopdf}
\usepackage{caption}
\usepackage{color}
\usepackage{mwe}
%\usepackage{colonequals}a
% Definitions of handy macros can go here

\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\usepackage{fancyhdr}
\usepackage{amsfonts, amsmath, amssymb, marvosym, upgreek, mathrsfs, dsfont} 
\usepackage{array,graphicx}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{setspace}
\usepackage{colonequals} 
\usepackage[authoryear]{natbib}

\renewcommand\harvardyearleft{\unskip, }
\renewcommand\harvardyearright[1]{.}
\newcommand\Fontvi{\fontsize{8}{7.2}\selectfont}
\newcommand{\vecX}{\mathbf{X}}
\newcommand{\vecZ}{\mathbf{Z}}
\newcommand{\vecU}{\mathbf{U}}
\newcommand{\vecI}{\mathbf{I}}
\newcommand{\vecO}{\mathbf{O}}
\newcommand{\vecE}{\mathbf{E}}
\newcommand{\vecM}{\mathbf{M}}
\newcommand{\vecS}{\mathbf{S}}
\newcommand{\vecT}{\mathbf{T}}
\newcommand{\vecx}{\mathbf{x}}
\newcommand{\vecu}{\mathbf{u}}
\newcommand{\vecz}{\mathbf{z}}
\newcommand{\veczero}{\mathbf{0}}
\newcommand{\vecmu}{\mbox{\boldmath$\mu$}}
\newcommand{\veczeta}{\mbox{\boldmath$\zeta$}}
\newcommand{\vecalpha}{\mbox{\boldmath$\alpha$}}
\newcommand{\vecbeta}{\mbox{\boldmath$\beta$}}
\newcommand{\vecgamma}{\mbox{\boldmath$\gamma$}}
\newcommand{\veclambda}{\mbox{\boldmath$\lambda$}}
\newcommand{\vecLambda}{\mbox{\boldmath$\Lambda$}}
\newcommand{\vecomega}{\mbox{\boldmath$\omega$}}
\newcommand{\vecpi}{\mbox{\boldmath$\pi$}}
\newcommand{\matsig}{\mbox{\boldmath$\Sigma$}}
\newcommand{\matTheta}{\mbox{\boldmath$\Theta$}}
\newcommand{\matepsilon}{\mbox{\boldmath$\epsilon$}}
\newcommand{\matPsi}{\mbox{\boldmath$\Psi$}}
\newcommand{\varthet}{\mbox{\boldmath$\vartheta$}}
\newcommand{\ttr}{\text{tr}}
\newcommand{\iprod}{\prod_{i=1}^n}
\newcommand{\isum}{\sum_{i=1}^n}
\newcommand{\gsum}{\sum_{g=1}^G}
\newcommand{\inv}{^{\raisebox{.2ex}{$\scriptscriptstyle-1$}}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\newtheorem{prop}{Proposition}
\hyphenation{McNicholas}


\begin{document}

\title{Supplemental: Finite Mixtures of Multivariate Poisson-Log Normal Factor Analyzers for Clustering Count Data}

\author{Anjali Silva\footnote{Department of Mathematics and Statistics, University of Guelph, Guelph, ON, Canada.}\qquad\qquad\qquad Steven~J.~Rothstein\footnote{Department of Molecular and Cellular Biology, University of Guelph, Guelph, ON, Canada.}\\\hspace{-0.4in}Paul~D.~McNicholas\footnote{Department of Mathematics and Statistics, McMaster University, Hamilton, ON, Canada.} \qquad\qquad Sanjeena Subedi\footnote{Department of Mathematical Sciences, Binghamton University, Binghamton, New York, USA.}}
\date{}

\maketitle


\noindent \textbf{Mathematical Details} \\
\noindent A $G$-component mixture of MPLN factor analyzers has the distribution
\begin{equation*}
\begin{split}
f(\mathbf{y};\boldsymbol{\Theta}) & = \sum_{g=1}^G\pi_gf_{\mathbf{Y}}(\mathbf{y}|\vecmu_g,\mathbf{\Lambda}_g, \mathbf{\Psi}_g)  \\
& = \sum_{g=1}^{G}  \pi_g \int_{\R^p} \left( \prod _{j=1}^p f(y_{ij}| \theta_{ijg}, s_j) \right) f(\boldsymbol{\theta}_{ig}|\vecmu_g,\mathbf{\Lambda}_g, \mathbf{\Psi}_g) ~d\boldsymbol{\theta}_{ig}.
\end{split}
\end{equation*}
Here $\boldsymbol{\Theta} = (\pi_1, \ldots, \pi_G, \vecmu_1, \ldots, \vecmu_G, \mathbf{\Lambda}_1, \ldots, \mathbf{\Lambda}_G, \mathbf{\Psi}_1, \ldots, \mathbf{\Psi}_G)$. At the first stage of the MCMC-AECM algorithm, when estimating $\boldsymbol{\vartheta}_1 =(\boldsymbol{\theta}_g, \pi_g, \vecmu_g; g = 1, \ldots, G)$, the group labels $z_{ig}$ and $\boldsymbol{\theta}_{ig}, i = 1,\ldots, n; g = 1,\ldots, G$ are the missing data. So the complete data likelihood for the mixture model is
\begin{equation*}
L_{c1}(\boldsymbol{\vartheta}_1) = \prod_{i=1}^{n} \prod_{g=1}^{G} \left[  \pi_g \left( \prod _{j=1}^p f(y_{ij}| \theta_{ijg}, s_j) \right) f(\boldsymbol{\theta}_{ig}|\vecmu_g,\mathbf{\Lambda}_g, \mathbf{\Psi}_g) ~d\boldsymbol{\theta}_{ig} \right]^{z_{ig}}. 
\end{equation*}
The complete data log-likelihood for the mixture model is
\begin{equation*}
\label{lc1}
\begin{split}
l_{c1}(\boldsymbol{\vartheta}_1) & = \sum_{i=1}^n \sum_{g=1}^G z_{ig} \Big[ \log \pi_g + \sum_{j=1}^p \log (f(y_{ij}|\theta_{ijg}, s_j)) + \log (f(\boldsymbol{\theta}_{ig}|\boldsymbol{\mu}_g,\mathbf{\Lambda}_g, \mathbf{\Psi}_g)) \Big], \\
& = 
\sum_{i=1}^n \sum_{g=1}^G z_{ig} \log \pi_g + \sum_{i=1}^n \sum_{g=1}^G \sum_{j=1}^p z_{ig} \Big( \log \big( \frac{\exp\{-\exp\{\theta_{ijg} + \log s_{j}\}\} (\exp\{\theta_{ijg} + \log s_{j}\})^{y_{ij}}}{y_{ij} !}\big) \Big) \\
& + \sum_{i=1}^n \sum_{g=1}^G z_{ig} \Big( \log \big(\frac{1}{\sqrt{(2\pi)^p | {\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g} |}} \exp\{-\frac{1}{2}[(\boldsymbol{\theta}_i - \boldsymbol{\mu}_g) (\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g)^{\inv} (\boldsymbol{\theta}_i - \boldsymbol{\mu}_g)^{\prime} ]\} \big) \Big), \\
& = \sum_{i=1}^n \sum_{g=1}^G z_{ig} \Big[ \log \pi_g + \sum_{j=1}^p  \log \big( \frac{\exp\{-\exp\{\theta_{ijg} + \log s_{j}\}\} (\exp\{\theta_{ijg} + \log s_{j}\})^{y_{ij}}}{y_{ij} !}\big) \\
& + \log \big(\frac{1}{\sqrt{(2\pi)^p | {\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g} |}} \exp\{-\frac{1}{2}[(\boldsymbol{\theta}_i - \boldsymbol{\mu}_g) (\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g)^{\inv} (\boldsymbol{\theta}_i - \boldsymbol{\mu}_g)^{\prime} ]\} \big) \Big],\\
& = \sum_{i=1}^n \sum_{g=1}^G  z_{ig} \Big[ \log \pi_g - \sum_{j=1}^p \exp\{\theta_{ijg} + \log s_{j}\} + \sum_{j=1}^p (\theta_{ijg} + \log s_j) y_{ij} - \sum_{j=1}^p \log y_{ij} ! \\
& - \frac{p}{2} \log 2 \pi - \frac{1}{2} \log | \mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g | - \frac{1}{2} (\boldsymbol{\theta}_i - \boldsymbol{\mu}_g) (\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g)^{\inv} (\boldsymbol{\theta}_i - \boldsymbol{\mu}_g)^{\prime}\Big],\\
& = \sum_{g=1}^G  n_g \log \pi_g - \sum_{i=1}^n \sum_{g=1}^G \sum_{j=1}^p z_{ig} \exp \{\theta_{ijg} + \log s_j\} + \sum_{i=1}^n \sum_{i=g}^G z_{ig}  (\boldsymbol{\theta}_{ig} + \log \mathbf{s}) \mathbf{y}_i^{\prime} \\
& - \sum_{i=1}^n \sum_{g=1}^G \sum_{j=1}^p z_{ig} \log y_{ij}! - \frac{np}{2} \log 2 \pi - \frac{1}{2} \sum_{g=1}^G n_g \log | \mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g| \\
& - \frac{1}{2} \sum_{g=1}^G n_g \text{tr} \big\{ \mathbf{S}_g \big( \mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g \big)^{\inv} \big\},
\end{split}
\end{equation*}
where $n_g = \sum_{i=1}^n z_{ig}$. Here, $\mathbf{S}_g$ represents the sample covariance matrix for component g, which has the form
\begin{equation*}
\label{sample_covariance_matrix}
     \small \mathbf{S}_g  = \frac{1}{n_g}\sum_{i=1}^n z_{ig} \E \Big((\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_g) (\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_g)^{\prime}\Big).
\end{equation*}
At each E-step, the (conditional) expected value of $\boldsymbol{\theta}_{ig}$ and (conditional) expected value of group membership variable, $\mathbf{z}_{ig}$, are updated as
\begin{equation}
\label{exptheta1}
\begin{split}
& \E( \boldsymbol{\theta}_{ig} |\textbf{y}_i) \simeq \frac{1} {N}\sum_{k=1}^N \boldsymbol{\theta}_{ig}^{(k)}  \simeq \boldsymbol{\theta}^{(t)}_{ig}, \\
& \E(Z_{ig} |\textbf{y}_i, \boldsymbol{\theta}_{ig}, \mathbf{s}) = \frac{\pi_g f(y_{ij}| \boldsymbol{\theta}_{ig}^{(t)}, \mathbf{s})  f(\boldsymbol{\theta}_{ig}|\boldsymbol{\mu}_g^{(t)},\mathbf{\Lambda}_g^{(t)}, \mathbf{\Psi}_g^{(t)})}{\sum_{h=1}^G \pi_h^{(t)} f(y_{ij}|\boldsymbol{\theta}_{ih}^{(t)}, \mathbf{s})  f(\boldsymbol{\theta}_{ih}| \boldsymbol{\mu}_h^{(t)},\mathbf{\Lambda}_h^{(t)}, \mathbf{\Psi}_h^{(t)})} =: z_{ig}^{(t)}.
\end{split}
\end{equation}
Note that, in the E-step, the estimates are conditioned on the current parameter estimates, hence the use of $(t)$ on the parameters in \eqref{exptheta1}. Using the expected values given by \eqref{exptheta1}, the expected value of the complete-data log-likelihood at first stage is
\begin{equation*} 
\begin{split}
\mathcal{Q}_1 & \simeq \sum_{g=1}^G n_g^{(t)} \log \pi_g^{(t)} - \sum_{i=1}^n \sum_{g=1}^G \sum_{j=1}^p z_{ig}^{(t)} \exp\{\E(\theta_{ijg}) + \log s_j\} + \sum_{i=1}^n \sum_{i=g}^G z_{ig}^{(t)} \big(\E(\boldsymbol{\theta}_{ig}) + \log \mathbf{s}\big) \mathbf{y}_i^{\prime} \\
& - \sum_{i=1}^n \sum_{g=1}^G \sum_{j=1}^p z_{ig} \log y_{ijg}! - \frac{np}{2} \log 2 \pi - \frac{1}{2} \sum_{g=1}^G n_g^{(t)} \log | \mathbf{\Lambda}_g^{(t)} \mathbf{\Lambda}_g^{(t) \prime} + \mathbf{\Psi}_g^{(t)} | \\
& - \frac{1}{2} \sum_{g=1}^G n_g^{(t)} \text{tr} \big\{ \mathbf{S}_g \big( \mathbf{\Lambda}_g^{(t)} \mathbf{\Lambda}_g^{(t) \prime} + \mathbf{\Psi}_g^{(t)} \big)^{\inv} \big\}.
\end{split}
\end{equation*}
where $n_g^{(t)} = \sum_{i=1}^n z_{ig}^{(t)}$. Here, $\mathbf{S}_g$ represents the sample covariance matrix for component g, which has the form
\begin{equation}
\label{sample_covariance_matrix2}
     \small \mathbf{S}_g  = \frac{1}{n_g^{(t)}} \sum_{i=1}^n z_{ig}^{(t)} \E \Big((\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_g^{(t)}) (\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_g^{(t)})^{\prime}\Big).
\end{equation}
Maximizing $\mathcal{Q}_1$ with respect to $\pi_g$ and $\boldsymbol{\mu}_g$ leads to the parameter updates,
\begin{equation*}
\begin{split}
	 &\small \pi_g^{(t+1)} = \frac{\sum	_{i=1}^n z_{ig}^{(t)}}{n} = \frac{n_g^{(t)}}{n} \\
     &\small \boldsymbol{\mu}^{(t+1)}_{g} = \frac{\sum	_{i=1}^n z_{ig}^{(t)} \E(\boldsymbol{\theta}_{ig})} {\sum_{i=1}^n z_{ig}^{(t)}} = \frac{\sum	_{i=1}^n z_{ig}^{(t)} \frac{1}{N} \sum_{k=1}^N\boldsymbol{\theta}_{ig}^{(k)}} {n_g^{(t)}}.
\end{split}
\end{equation*}
\noindent At the second stage of the MCMC-AECM algorithm, when estimating $\boldsymbol{\vartheta}_2 =(\boldsymbol{\Lambda}_g, \boldsymbol{\Psi}_g; g = 1, \ldots, G)$, the group labels $\mathbf{z}_i$, the latent factors $\mathbf{u}_{ig}$ and $\boldsymbol{\theta}_{ig},i = 1, \ldots, n; g = 1, \ldots, G$, are taken to be the missing data. Here, $\boldsymbol{\theta}_{ig} | \mathbf{u}_{ig} \sim N(\boldsymbol{\mu}_g + \mathbf{\Lambda}_g \mathbf{u}_{ig}, \mathbf{\Psi}_g)$ and $\vecu_{ig} \sim \mathcal{N}(\veczero,\vecI_q)$. The complete data likelihood is 
\begin{equation*}
\begin{split}
L_{c2}(\boldsymbol{\vartheta}_2) = \prod_{i=1}^{n} \left[\prod_{g=1}^{G} \pi_g^{(t+1)} \left(\prod_{j=1}^{p} f(y_{ij}|\theta_{ijg}^{(t)}, s_j) \right)f(\boldsymbol{\theta}_{ig}| \mathbf{u}_i, \boldsymbol{\mu}_g^{(t+1)},\mathbf{\Lambda}_g^{(t)}, \mathbf{\Psi}_g^{(t)}) f(\mathbf{u}_i| \mathbf{0}, \mathbf{I}_q) \right]^{z_{ig}^{(t)}},
\end{split}
\end{equation*}
and the complete data log-likelihood is
\begin{equation*}
\begin{split}
l_{c2}(\boldsymbol{\vartheta}_2) &= \sum_{i=1}^n \sum_{g=1}^G z_{ig}^{(t)} \log \left[ \pi_g^{(t+1)} \left( \prod_{j=1}^p f(y_{ij}|\theta_{ijg}^{(t)}, s_j) \right) f(\boldsymbol{\theta}_{ig}| \mathbf{u}_i, \boldsymbol{\mu}_g^{(t+1)},\mathbf{\Lambda}_g^{(t)}, \mathbf{\Psi}_g^{(t)}) f(\mathbf{u}_i| \mathbf{0}, \mathbf{I}_q) \right], \\
& = \sum_{i=1}^n \sum_{g=1}^G z_{ig}^{(t)} \Bigg[ \log \pi_g^{(t+1)} + \left( \sum_{j=1}^p \log f(y_{ij}|\theta_{ijg}^{(t)}, s_j) \right) + \log f(\boldsymbol{\theta}_{ig}| \mathbf{u}_i, \boldsymbol{\mu}_g^{(t+1)},\mathbf{\Lambda}_g^{(t)}, \mathbf{\Psi}_g^{(t)}) \\
& + \log f(\mathbf{u}_i| \mathbf{0}, \mathbf{I}_q) \Bigg], \\
& = \sum_{g=1}^G \Big[ n_g^{(t)} \log \pi_g^{(t+1)} - \sum_{i=1}^n \sum_{j=1}^p z_{ig}^{(t)} \exp\{\theta_{ijg}^{(t)} + \log s_j\} + \sum_{i=1}^n z_{ig}  (\boldsymbol{\theta}_{ig}^{(t)} + \log \mathbf{s}) \mathbf{y}_i^{\prime} \\
& - \sum_{i=1}^n \sum_{j=1}^p z_{ig}^{(t)} \log y_{ij} ! -\frac{n_g}{2} \log | \mathbf{\Psi}_g^{(t)} |  - \frac{n_g}{2} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)} - \mathbf{\Lambda}_g^{(t)} \mathbf{u}_i)^{\prime} \mathbf{\Psi}_g^{^{(t)} \inv} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)} - \mathbf{\Lambda}_g^{(t)} \mathbf{u}_i) \Big], \\
& = \sum_{g=1}^G \Big[ n_g^{(t)} \log \pi_g^{(t+1)} - \sum_{i=1}^n \sum_{j=1}^p z_{ig}^{(t)} \exp\{\theta_{ijg}^{(t)} + \log s_j \} + \sum_{i=1}^n z_{ig}^{(t)}  (\boldsymbol{\theta}_{ig}^{(t)} + \log \mathbf{s}) \mathbf{y}_i^{\prime} \\
& - \sum_{i=1}^n  \sum_{j=1}^p z_{ig}^{(t)} \log y_{ij} ! -\frac{n_g^{(t)}}{2} \log | \mathbf{\Psi}_g^{(t)} | - \frac{n_g^{(t)}}{2} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}^{(t+1)})^{\prime} \mathbf{\Psi}_g^{(t) \inv} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)}) \\
& + \sum_{i=1}^n z_{ig}^{(t)} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)})^{\prime} \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \mathbf{u}_i - \frac{1}{2} \sum_{i=1}^n \mathbf{u}_i^{\prime} \mathbf{\Lambda}_g^{(t) \prime} \mathbf{\Psi}_g^{(t)  \inv} \mathbf{\Lambda}_g^{(t)} \mathbf{u}_i \Big], \\
& = \sum_{g=1}^G \Big[ n_g^{(t)} \log \pi_g^{(t+1)} - \sum_{i=1}^n \sum_{j=1}^p z_{ig}^{(t)} \exp\{\theta_{ijg}^{(t)} + \log s_j\} + \sum_{i=1}^n z_{ig}^{(t)} (\boldsymbol{\theta}_g^{(t)} + \log \mathbf{s}) \mathbf{y}_i^{\prime} \\
& - \sum_{i=1}^n \sum_{j=1}^p z_{ig}^{(t)} \log y_{ij}! - \frac{n_g^{(t)}}{2} \log | \mathbf{\Psi}_g^{(t)} | - \frac{n_g^{(t)}}{2} \text{tr} \{\mathbf{\Psi}_g^{(t)  \inv} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)}) (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)})^{\prime} \} \\
&  +\sum_{i=1}^n z_{ig}^{(t)} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)})^{\prime} \mathbf{\Psi}_g^{(t)  \inv} \mathbf{\Lambda}_g^{(t)} \mathbf{u}_i - \frac{1}{2} \text{tr} \{\mathbf{\Lambda}_g^{(t) \prime} \mathbf{\Psi}_g^{(t)  \inv} \mathbf{\Lambda}_g^{(t)} \sum_{i=1}^n \mathbf{u}_i \mathbf{u}_i^{\prime} \} \Big], \\
& = C + \sum_{g=1}^G \Bigg[ -\frac{n_g^{(t)}}{2} \log | \mathbf{\Psi}_g^{(t)} | - \frac{n_g^{(t)}}{2}\text{tr} \{\mathbf{\Psi}_g^{(t)  \inv} \mathbf{S}_g \} + \sum_{i=1}^n z_{ig}^{(t)} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)})^{\prime} \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \mathbf{u}_i \\
& - \frac{1}{2} \text{tr} \Big\{ \mathbf{\Lambda}_g^{(t) \prime} \mathbf{\Psi}_g^{(t)  \inv} \mathbf{\Lambda}_g^{(t)} \sum_{i=1}^n z_{ig}^{(t)} \mathbf{u}_i  \mathbf{u}_i^{\prime} \Big\} \Bigg],
\end{split}
\end{equation*}
where C is a constant with respect to $\mathbf{\Lambda}_g$ and $\mathbf{\Psi}_g$. The expected value of the complete-data log-likelihood at second stage is
\begin{equation}
\label{Q2_nonreduced}
\begin{split}
\
\mathcal{Q}_2 & \simeq C + \sum_{g=1}^G \Big( -\frac{n_g^{(t)}}{2} \log | \mathbf{\Psi}_g^{(t)} | - \frac{n_g^{(t)}}{2}\text{tr} \{\mathbf{\Psi}_g^{(t) \inv} \mathbf{S}_g \} \\
& + \sum_{i=1}^n z_{ig}^{(t)} (\boldsymbol{\theta}_i^{(t)} - \boldsymbol{\mu}_g^{(t+1)})^{\prime} \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \E(\mathbf{u}_i | \boldsymbol{\theta}_i^{(t)}, z_{ig}^{(t)} =1) \\ 
& - \frac{1}{2} \text{tr} \Big\{ \mathbf{\Lambda}_g^{(t) \prime} \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \sum_{i=1}^n z_{ig}^{(t)} \E(\mathbf{u}_i  \mathbf{u}_i^{\prime} | \boldsymbol{\theta}_i^{(t)}, z_{ig}^{(t)} =1) \Big\} \Big).
\end{split}
\end{equation} 

\noindent Consider the joint distribution of $\boldsymbol{\theta}_{ig}$ and $\mathbf{u}_{ig}$,
\begin{equation*}
\[ 
\left[ \begin{array}{ccc}
\boldsymbol{\theta}_{ig} \\
\mathbf{U}_{ig}
\end{array} \right]
%
\sim N \left( \begin{array}{c}
\left[ \begin{array}{c}
\boldsymbol{\mu}_g \\
\mathbf{0}
\end{array} \right],
%
 \left[ \begin{array}{cc}
\mathbf{\Lambda}_g\mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g \hspace{4mm} \mathbf{\Lambda}_g\\
\mathbf{\Lambda}_g^{\prime} \hspace{17mm} \vecI_q
\end{array} \right]
\end{array} \right).
\]
\end{equation*} 

\noindent Given this information, the following can be calculated as $$\E(\mathbf{U}_{ig} | \boldsymbol{\theta}_{ig}) = \boldsymbol{\mu}_{u_{ig}u_{ig}} + \boldsymbol{\Sigma}_{u_{ig} \theta_{ig}}\boldsymbol{\Sigma}_{\theta_{ig}\theta_{ig}}^{\inv}(\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_{\theta_{ig}}),$$ and $$\mathbb{V}\text{ar}(\mathbf{U}_{ig} | \boldsymbol{\theta}_{ig}) = \boldsymbol{\Sigma}_{u_{ig}u_{ig}} - \boldsymbol{\Sigma}_{u_{ig}\theta_{ig}}\boldsymbol{\Sigma}_{\theta_{ig}\theta_{ig}}^{\inv}\boldsymbol{\Sigma}_{\theta_{ig} u_{ig}}.$$ Accordingly, this results
\begin{equation*}
\begin{split}
& \E(\mathbf{U}_{ig} | \boldsymbol{\theta}_{ig}) = \mathbf{\Lambda}_g^{\prime} \big(\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g)^{\inv}  \big(\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_g \big)  \\
& \E(\mathbf{U}_{ig}  \mathbf{U}_{ig}^{\prime} | \boldsymbol{\theta}_{ig}, z_{i} =1) = \mathbb{V}\text{ar} (\mathbf{U}_{ig} |\boldsymbol{\theta}_{ig}) + \E(\mathbf{U}_{ig} |\boldsymbol{\theta}_{ig}) \E(\mathbf{U}_{ig} |\boldsymbol{\theta}_{ig})^{\prime} \\
& \hspace{20mm} = \big(\vecI_q + \mathbf{\Lambda}_g^{\prime}  \mathbf{\Psi}_g^{\inv} \mathbf{\Lambda}_g \big)^{\inv} +  
\Big( \mathbf{\Lambda}_g^{\prime} \big(\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g)^{\inv}  \big(\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_g \big)\Big) \Big( \mathbf{\Lambda}_g^{\prime} \big(\mathbf{\Lambda}_g \mathbf{\Lambda}_g^{\prime} + \mathbf{\Psi}_g)^{\inv}  \big(\boldsymbol{\theta}_{ig} - \boldsymbol{\mu}_g \big) \Big)^{\prime} 
 \end{split}
\end{equation*} 
Therefore, \eqref{Q2_nonreduced} can be written as 
\begin{equation}
\label{Q2_reduced}
\begin{split}
\mathcal{Q}_2 & \simeq C + \frac{1}{2} \sum_{g=1}^G n_g^{(t)} \Big[ \log |\mathbf{\Psi}_g^{(t) \inv}| - \text{tr} \big\{\mathbf{\Psi}_g^{(t) \inv} \mathbf{S}_g\big\} + 2 \text{tr} \big\{\mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \big\} - \text{tr}\big\{\mathbf{\Lambda}_g^{(t) \prime} \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)}  \mathbf{\Phi}_g^{(t)} \big\} \Big], \\
& \simeq C + \frac{n}{2} \Big[ \log |\mathbf{\Psi}_g^{(t) \inv}| - \text{tr} \big\{\mathbf{\Psi}_g^{(t) \inv} \mathbf{S}_g\big\} + 2 \text{tr} \big\{\mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)}  \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \big\} - \text{tr}\big\{\mathbf{\Lambda}_g^{(t) \prime} \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)}  \mathbf{\Phi}_g^{(t)}  \big\} \Big],
\end{split}
\end{equation} 
where $\sum_{g=1}^G n_g^{(t)} = n$, $\hat{\boldsymbol{\beta}}_g$ is a $q \times p$ matrix is given by
\begin{equation*}
\hat{\boldsymbol{\beta}}_g^{(t)} = \mathbf{\Lambda}_g^{(t) \prime} \big(\mathbf{\Lambda}_g^{(t)} \mathbf{\Lambda}_g^{(t) \prime} + \mathbf{\Psi}_g^{(t)}\big)^{\inv},
\end{equation*}
and $\mathbf{\Phi}_g$ is a symmetric $q \times q$ matrix given by
\begin{equation*}
\mathbf{\Phi}_g^{(t)} = \vecI_q - \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{\Lambda}_g^{(t)} + \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime}.
\end{equation*} 
Note, the $\boldsymbol{\mu}_g^{(t+1)}$ replaces $\boldsymbol{\mu}_g^{(t)}$ in $\mathbf{S}_g$, cf. \eqref{sample_covariance_matrix2}. Differentiating \eqref{Q2_reduced} with respect to $\mathbf{\Lambda}_g$ and $\mathbf{\Psi}_g^{\inv}$, respectively, leads to 
\begin{equation*}
S_1(\mathbf{\Lambda}_g, \mathbf{\Psi}_g) = \frac{\partial \mathcal{Q}_2(\mathbf{\Lambda}_g, \mathbf{\Psi}_g)}{\partial \mathbf{\Lambda}_g} =  \sum_{g=1}^G n_g^{(t)} \Big[ \mathbf{\Psi}_g^{(t) \inv} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime} - \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \mathbf{\Phi}_g^{(t)} \Big],
\end{equation*}
\begin{equation*}
S_2(\mathbf{\Lambda}_g, \mathbf{\Psi}_g) = \frac{\partial \mathcal{Q}_2(\mathbf{\Lambda}_g, \mathbf{\Psi}_g)}{\partial \mathbf{\Psi}_g^{\inv}} =  \frac{n_g^{(t)}}{2} \Big[ \mathbf{\Psi}_g^{(t)} - \mathbf{S}_g^{\prime} + 2 \mathbf{\Lambda}_g^{(t)} \hat{\boldsymbol{\beta}}_g^{(t)}  \mathbf{S}_g - \mathbf{\Lambda}_g^{(t)} \mathbf{\Phi}_g^{\prime}^{(t)} \mathbf{\Lambda}_g^{(t) \prime} \Big].
\end{equation*}
Solving $S_1(\mathbf{\Lambda}_g^{(t+1)}, \mathbf{\Psi}_g^{(t+1)}) = 0$ and $\text{diag}\{S_2(\mathbf{\Lambda}_g^{(t+1)}, \mathbf{\Psi}_g^{(t+1)})\} = 0$, leads to
\begin{equation*}
\mathbf{\Lambda}_g^{(t+1)} = \mathbf{S}_g \hat{\boldsymbol{\beta}}
_g^{(t) \prime}{\mathbf{\Phi}}_g^{(t) \inv},
\end{equation*}
and
\begin{equation*}
\mathbf{\Psi}_g^{(t+1)} = \text{diag}\{\mathbf{S}_g - \mathbf{\Lambda}_g^{(t+1)}\hat{\boldsymbol{\beta}}_g^{(t)}\mathbf{S}_g\}.
\end{equation*}
The form of complete-data log-likelihood and the parameter estimates will vary depending on which of the four models in the PMPLNFA family is under consideration. \\

\noindent \textbf{If equal loading matrices}: $\mathbf{\Lambda}_g = \mathbf{\Lambda}$ are assumed, then \eqref{Q2_reduced} can be written as
\begin{equation}
\label{Q2_lambda_constrained}
\begin{split}
\mathcal{Q}_2 & \simeq C + \frac{1}{2} \sum_{g=1}^G n_g^{(t)} \Big[ \log |\mathbf{\Psi}_g^{(t) \inv}| - \text{tr} \big\{\mathbf{\Psi}_g^{(t) \inv} \mathbf{S}_g \big\} + 2 \text{tr} \big\{\mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}^{(t)} \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \big\} - \text{tr}\big\{\mathbf{\Lambda}^{(t) \prime} \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}^{(t)} \mathbf{\Phi}_g^{(t)} \big\} \Big],
\end{split}
\end{equation} 
where $\hat{\boldsymbol{\beta}}_g^{(t)} = \mathbf{\Lambda}^{(t)\prime} \big(\mathbf{\Lambda}^{(t)} \mathbf{\Lambda}^{(t) \prime} + \mathbf{\Psi}_g^{(t)} \big)^{\inv}$ and $\mathbf{\Phi}_g^{(t)} = \vecI_q - \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{\Lambda}^{(t)} +  \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime}$. Differentiating \eqref{Q2_lambda_constrained} with respect to $\mathbf{\Lambda}^{(t)}$ and $\mathbf{\Psi}_g^{(t) \inv}$, respectively, leads to
\begin{equation*}
S_3(\mathbf{\Lambda}, \mathbf{\Psi}_g) = \frac{\partial \mathcal{Q}_2(\mathbf{\Lambda}, \mathbf{\Psi}_g)}{\partial \mathbf{\Lambda}} = \sum_{g=1}^G n_g^{(t)}  \Big[\mathbf{\Psi}_g^{(t) \inv} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime} - \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}^{(t)} \mathbf{\Phi}_g^{(t)}\Big],
\end{equation*}
\begin{equation*}
S_4(\mathbf{\Lambda}, \mathbf{\Psi}_g) = \frac{\partial \mathcal{Q}_2(\mathbf{\Lambda}, \mathbf{\Psi}_g)}{\partial \mathbf{\Psi}_g^{\inv}} = \frac{n_g^{(t)}}{2} \Big[ \mathbf{\Psi}_g^{(t)} - \mathbf{S}_g^{\prime} + 2 \mathbf{\Lambda}^{(t)} \hat{\boldsymbol{\beta}}_g^{(t)}  \mathbf{S}_g - \mathbf{\Lambda}^{(t)} \mathbf{\Phi}_g^{(t) \prime} \mathbf{\Lambda}^{(t) \prime} \Big]. 
\end{equation*}
Setting $S_3(\mathbf{\Lambda}^{(t+1)}, \mathbf{\Psi}_g^{(t+1)}) = 0$ leads to 
\begin{equation}
\label{solvingS3}
\sum_{g=1}^G n_g \mathbf{\Psi}_g^{(t) \inv} \mathbf{\Lambda}^{(t+1)} \mathbf{\Phi}_g^{(t)} = \sum_{g=1}^G n_g^{(t)} \mathbf{\Psi}_g^{(t) \inv} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime},
\end{equation}
which must be solved for $\mathbf{\Lambda}^{(t+1)}$ in a row-by-row manner. This slows the fitting of this model. Let $\mathbf{\lambda}_i^{(t)} = \big( \mathbf{\lambda}_{i1}, \mathbf{\lambda}_{i2}, \ldots, \mathbf{\lambda}_{i1} \big)$ represent the $i$th row of the matrix $\mathbf{\Lambda}$ and let $r_i$ represent the $i$th row of the matrix on the right-hand side of \eqref{solvingS3}. Here $i$th row of \eqref{solvingS3} can be written as 
\begin{equation*}
\mathbf{\lambda}_i^{(t+1)} \sum_{g=1}^G \frac{n_g^{(t)} }{\psi^{(t)}_{g(i)}} \mathbf{\Phi}_g^{(t)} = \mathbf{r}_i,
\end{equation*}
where $\psi^{(t)}_{g(i)}$ is the $i$th entry along the diagonal of $\mathbf{\Psi}_g^{(t)} $. Hence, 
\begin{equation*}
\mathbf{\lambda}_i^{(t+1)}  = \mathbf{r}_i \Big( \sum_{g=1}^G \frac{n_g^{(t)}}{\psi^{(t)}_{g(i)}} \mathbf{\Phi}_g^{(t)} \Big)^{\inv},
\end{equation*}
for $i = 1, \ldots, p$. \\

\noindent Setting $\text{diag}\{S_4(\mathbf{\Lambda}^{(t+1)}, \mathbf{\Psi}_g^{(t+1)})\} = 0$ and solving leads to 
\begin{equation*}
\mathbf{\Psi}_g^{(t+1)} = \text{diag}\{\mathbf{S}_g - 2 \mathbf{\Lambda}^{(t+1)}\hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g + \mathbf{\Lambda}^{(t+1)} \mathbf{\Phi}_g^{(t)} ( \mathbf{\Lambda}^{(t+1)})^{\prime} \}.
\end{equation*}
\\
\noindent \textbf{If equal error variance}: $\mathbf{\Psi}_g = \mathbf{\Psi}$ are assumed, then \eqref{Q2_reduced} can be written as
\begin{equation}
\label{Q2_psi_constrained}
\begin{split}
\mathcal{Q}_2 & \simeq C + \frac{1}{2} \sum_{g=1}^G n_g^{(t)} \Big[ \log |\mathbf{\Psi}^{(t) \inv}| - \text{tr} \big\{\mathbf{\Psi}^{(t) \inv} \mathbf{S}_g\big\} + 2 \text{tr} \big\{\mathbf{\Psi}^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \big\} - \text{tr}\big\{\mathbf{\Lambda}_g^{(t) \prime} \mathbf{\Psi}^{(t) \inv} \mathbf{\Lambda}_g^{(t)} \mathbf{\Phi}_g^{(t)} \big\} \Big],
\end{split}
\end{equation} 
where $\hat{\boldsymbol{\beta}}_g^{(t)} = \mathbf{\Lambda}_g^{(t) \prime} \big(\mathbf{\Lambda}_g^{(t)} \mathbf{\Lambda}_g^{(t) \prime} + \mathbf{\Psi}^{(t)} \big)^{\inv}$ and $\mathbf{\Phi}_g^{(t)} = \vecI_q - \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{\Lambda}_g^{(t)} +  \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime}$. Differentiating \eqref{Q2_psi_constrained} with respect to $\mathbf{\Lambda}_g^{(t)}$ and $\mathbf{\Psi}^{(t) \inv}$, respectively, leads to
\begin{equation*}
S_5(\mathbf{\Lambda}_g, \mathbf{\Psi}) = \frac{\partial \mathcal{Q}_2(\mathbf{\Lambda}_g, \mathbf{\Psi})}{\partial \mathbf{\Lambda}_g} = \sum_{g=1}^G n_g^{(t)} \mathbf{\Psi}^{(t) \inv} \Big[ \mathbf{S}_g \hat{\mathbf{\beta}}_g^{(t) \prime} - \mathbf{\Lambda}_g^{(t)} \mathbf{\Phi}_g^{(t)}\Big],
\end{equation*}
\begin{equation*}
S_6(\mathbf{\Lambda}_g, \mathbf{\Psi}) = \frac{\partial \mathcal{Q}_2(\mathbf{\Lambda}_g, \mathbf{\Psi})}{\partial \mathbf{\Psi}^{\inv}} = \frac{n}{2} \mathbf{\Psi}^{(t)} - \frac{1}{2} \sum_{g=1}^G n_g^{(t)} \Big[ \mathbf{S}_g^{\prime} - 2 \mathbf{\Lambda}_g^{(t)} \hat{\boldsymbol{\beta}}_g^{(t)}  \mathbf{S}_g + \mathbf{\Lambda}_g^{(t)} \mathbf{\Phi}_g^{(t) \prime} \mathbf{\Lambda}_g^{(t) \prime} \Big]. 
\end{equation*}
Setting $S_5 (\mathbf{\Lambda}_g^{(t+1)}, \mathbf{\Psi}^{(t+1)}) = 0$ leads to 
\begin{equation*}
\mathbf{\Lambda}_g^{(t+1)} = \mathbf{S}_g \hat{\boldsymbol{\beta}}
_g^{(t) \prime}{\mathbf{\Phi}}_g^{(t) \inv},
\end{equation*}
and setting $S_6(\mathbf{\Lambda}_g^{(t+1)}, \mathbf{\Psi}^{(t+1)}) = 0$ leads to 
\begin{equation*}
\mathbf{\Psi}^{(t+1)} = \sum_{g=1}^G \pi_g^{(t)} \text{diag}\{\mathbf{S}_g - \mathbf{\Lambda}_g^{(t+1)}\hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g\}.
\end{equation*}
\\
\noindent By combining these constraints, a set of four models can be obtained as described in Table 4.1, where ``constrained" means $\mathbf{\Lambda}_g  =\mathbf{\Lambda} $ in the loading matrix term and $ \mathbf{\Psi}_g =  \mathbf{\Psi}$  in the error variance term. \\

\noindent Estimation procedure for four covariance structures: \\
\textbf{Model UU}: no constraint is assumed.
\begin{equation*}
\hat{\boldsymbol{\beta}}_g^{(t)} = \mathbf{\Lambda}_g^{(t) \prime} \big(\mathbf{\Lambda}_g^{(t)} \mathbf{\Lambda}_g^{(t) \prime} + \mathbf{\Psi}_g^{(t)} \big)^{\inv},
\end{equation*}
\begin{equation*}
\mathbf{\Phi}_g^{(t)} = \vecI_q - \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{\Lambda}_g^{(t)} +  \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime},
\end{equation*}
\begin{equation*}
\mathbf{\Lambda}_g^{(t+1)} = \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime}{\mathbf{\Phi}}_g^{(t) \inv},
\end{equation*}
\begin{equation*}
\mathbf{\Psi}_g^{(t+1)} = \text{diag}\{\mathbf{S}_g - \mathbf{\Lambda}_g^{(t+1)}\hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g\}.
\end{equation*}

\textbf{Model CU}: Assume $\mathbf{\Lambda}_g = \mathbf{\Lambda}$.
\begin{equation*}
\hat{\boldsymbol{\beta}}_g^{(t)} = \mathbf{\Lambda}^{(t) \prime} \big(\mathbf{\Lambda}^{(t)} \mathbf{\Lambda}^{(t) \prime} + \mathbf{\Psi}_g^{(t)} \big)^{\inv},
\end{equation*}
\begin{equation*}
\mathbf{\Phi}_g^{(t)} = \vecI_q - \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{\Lambda}^{(t)} +  \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime},
\end{equation*}
\begin{equation*}
\mathbf{\lambda}_i^{(t+1)}  = \mathbf{r}_i \Big( \sum_{g=1}^G \frac{n_g^{(t)}}{\psi^{(t)}_{g(i)}} \mathbf{\Phi}_g^{(t)} \Big)^{\inv},
\end{equation*}
\begin{equation*}
\mathbf{\Psi}_g^{(t+1)} = \text{diag}\{\mathbf{S}_g - 2 \mathbf{\Lambda}^{(t+1)}\hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g + \mathbf{\Lambda}^{(t+1)} \mathbf{\Phi}_g^{(t)} ( \mathbf{\Lambda}^{(t+1)})^{\prime} \}.
\end{equation*}

\textbf{Model UC}: Assume $\mathbf{\Psi}_g = \mathbf{\Psi}$.
\begin{equation*}
\hat{\boldsymbol{\beta}}_g^{(t)} = \mathbf{\Lambda}_g^{(t) \prime} \big(\mathbf{\Lambda}_g^{(t)} \mathbf{\Lambda}_g^{(t) \prime} + \mathbf{\Psi}^{(t)} \big)^{\inv},
\end{equation*}
\begin{equation*}
\mathbf{\Phi}_g^{(t)} = \vecI_q - \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{\Lambda}_g^{(t)} +  \hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime},
\end{equation*}
\begin{equation*}
\mathbf{\Lambda}_g^{(t+1)} = \mathbf{S}_g \hat{\boldsymbol{\beta}}_g^{(t) \prime}{\mathbf{\Phi}}_g^{\inv},
\end{equation*}
\begin{equation*}
\mathbf{\Psi}^{(t+1)} = \sum_{g=1}^G \pi_g^{(t)} \text{diag}\{\mathbf{S}_g - \mathbf{\Lambda}_g^{(t+1)}\hat{\boldsymbol{\beta}}_g^{(t)} \mathbf{S}_g\}.
\end{equation*}

\textbf{Model CC}: Assume $\mathbf{\Lambda}_g = \mathbf{\Lambda}$ and $\mathbf{\Psi}_g = \mathbf{\Psi}$. Here $\tilde{\mathbf{S}} = \sum_{g=1}^G \pi_g^{(t)} \mathbf{S}_g$.
\begin{equation*}
\hat{\boldsymbol{\beta}}^{(t)} = \mathbf{\Lambda}^{(t) \prime} \big(\mathbf{\Lambda}^{(t)} \mathbf{\Lambda}^{(t) \prime} + \mathbf{\Psi}^{(t)} \big)^{\inv},
\end{equation*}
\begin{equation*}
\tilde{\mathbf{\Phi}}^{(t)} = \vecI_q - \hat{\boldsymbol{\beta}}^{(t)} \mathbf{\Lambda}^{(t)} +  \hat{\boldsymbol{\beta}}^{(t)} \tilde{\mathbf{S}} \hat{\boldsymbol{\beta}}^{(t) \prime},
\end{equation*}
\begin{equation*}
\mathbf{\Lambda}^{(t+1)} = \tilde{\mathbf{S}} \hat{\boldsymbol{\beta}}^{(t) \prime} \tilde{\mathbf{\Phi}}^{\inv},
\end{equation*}
\begin{equation*}
\mathbf{\Psi}^{(t+1)} = \text{diag}\{\tilde{\mathbf{S}} - \mathbf{\Lambda}^{(t+1)}\hat{\boldsymbol{\beta}}^{(t)} \tilde{\mathbf{S}}\}.
\end{equation*}


\end{document}
